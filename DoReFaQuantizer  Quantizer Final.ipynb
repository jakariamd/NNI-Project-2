{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559a9102",
   "metadata": {},
   "source": [
    "# Load Pretrained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63519f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from nni.compression.pytorch.speedup import ModelSpeedup\n",
    "from nni.compression.pytorch.utils import count_flops_params\n",
    "import time\n",
    "\n",
    "from mnist_model import Net, train, test, device, optimizer_scheduler_generator, trainer, calibration\n",
    "\n",
    "# Load pretrained model\n",
    "model = torch.load(\"mnist_cnn.pt\")\n",
    "model.eval()\n",
    "\n",
    "# show the model stbructure, note that pruner will wrap the model layer.\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4435816",
   "metadata": {},
   "source": [
    "### Performance and statistics of pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f65e18e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9919/10000 (99.19%)\n",
      "\n",
      "Pretrained model Accuracy:  99.19%, Test-time:  1.6945s\n"
     ]
    }
   ],
   "source": [
    "## Performance \n",
    "start = time.time()\n",
    "pre_best_acc = test(model, device)\n",
    "pre_test_time = time.time() - start\n",
    "\n",
    "print(f'Pretrained model Accuracy: {pre_best_acc: .2f}%, Test-time: {pre_test_time: .4f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba20bd",
   "metadata": {},
   "source": [
    "# Quantizing Model  with QAT Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fb9e4",
   "metadata": {},
   "source": [
    "## Configuration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c72f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining  configuration List\n",
    "config_list = [{\n",
    "        'quant_types': ['weight'],\n",
    "        'quant_bits': {\n",
    "            'weight': 8,\n",
    "        },\n",
    "        'op_types':['Conv2d']\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9324afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): QuantizerModuleWrapper(\n",
       "    (module): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (conv2): QuantizerModuleWrapper(\n",
       "    (module): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Construct the DoReFaQuantizer.\n",
    "from nni.algorithms.compression.pytorch.quantization import DoReFaQuantizer\n",
    "optimizer, scheduler = optimizer_scheduler_generator(model)\n",
    "quantizer = DoReFaQuantizer(model, config_list, optimizer)\n",
    "quantizer.compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03805b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0608, Accuracy: 9866/10000 (98.66%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0508, Accuracy: 9882/10000 (98.82%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0518, Accuracy: 9880/10000 (98.80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Finetune the model \n",
    "total_epoch = 3  \n",
    "optimizer, scheduler = optimizer_scheduler_generator(model)\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "        train(model, device, optimizer=optimizer, epoch=epoch)\n",
    "        test(model, device)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0fd4bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-11 17:44:01] \u001b[32mModel state_dict saved to ./log/mnist_model.pth\u001b[0m\n",
      "[2022-10-11 17:44:01] \u001b[32mMask dict saved to ./log/mnist_calibration.pth\u001b[0m\n",
      "calibration_config:  {'conv1': {'weight_bits': 8}, 'conv2': {'weight_bits': 8}}\n"
     ]
    }
   ],
   "source": [
    "## export model and get calibration_config\n",
    "\n",
    "model_path = \"./log/mnist_model.pth\"\n",
    "calibration_path = \"./log/mnist_calibration.pth\"\n",
    "calibration_config = quantizer.export_model(model_path, calibration_path)\n",
    "\n",
    "print(\"calibration_config: \", calibration_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef3636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0518, Accuracy: 9880/10000 (98.80%)\n",
      "\n",
      "Pretrained model Accuracy:  98.80%, Test-time:  1.6796s, Speed-up:  1.01x\n"
     ]
    }
   ],
   "source": [
    "## Performance \n",
    "start = time.time()\n",
    "best_acc = test(model, device)\n",
    "test_time = time.time() - start\n",
    "\n",
    "print(f'Pretrained model Accuracy: {best_acc: .2f}%, Test-time: {test_time: .4f}s, Speed-up: {pre_test_time/test_time: .2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61af61a",
   "metadata": {},
   "source": [
    "## Configuration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5209f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = torch.load(\"mnist_cnn.pt\")\n",
    "model.eval()\n",
    "\n",
    "config_list = [{\n",
    "    'op_types': ['Conv2d'],\n",
    "    'quant_types': ['weight'],\n",
    "    'quant_bits': {'weight': 8}\n",
    "}, {\n",
    "    'op_names': ['fc1', 'fc2'],\n",
    "    'quant_types': [ 'weight'],\n",
    "    'quant_bits': {'weight': 8}\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ece5920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): QuantizerModuleWrapper(\n",
       "    (module): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (conv2): QuantizerModuleWrapper(\n",
       "    (module): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): QuantizerModuleWrapper(\n",
       "    (module): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  )\n",
       "  (fc2): QuantizerModuleWrapper(\n",
       "    (module): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Construct the DoReFaQuantizer.\n",
    "from nni.algorithms.compression.pytorch.quantization import DoReFaQuantizer\n",
    "optimizer, scheduler = optimizer_scheduler_generator(model)\n",
    "quantizer = DoReFaQuantizer(model, config_list, optimizer)\n",
    "quantizer.compress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d60ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3281, Accuracy: 8821/10000 (88.21%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1447, Accuracy: 9819/10000 (98.19%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.1627, Accuracy: 9851/10000 (98.51%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Finetune the model \n",
    "total_epoch = 3  \n",
    "optimizer, scheduler = optimizer_scheduler_generator(model)\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "        train(model, device, optimizer=optimizer, epoch=epoch)\n",
    "        test(model, device)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c0a9abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-11 17:46:10] \u001b[32mModel state_dict saved to ./log/mnist_model.pth\u001b[0m\n",
      "[2022-10-11 17:46:10] \u001b[32mMask dict saved to ./log/mnist_calibration.pth\u001b[0m\n",
      "calibration_config:  {'conv1': {'weight_bits': 8}, 'conv2': {'weight_bits': 8}, 'fc1': {'weight_bits': 8}, 'fc2': {'weight_bits': 8}}\n"
     ]
    }
   ],
   "source": [
    "## export model and get calibration_config\n",
    "\n",
    "model_path = \"./log/mnist_model.pth\"\n",
    "calibration_path = \"./log/mnist_calibration.pth\"\n",
    "calibration_config = quantizer.export_model(model_path, calibration_path)\n",
    "\n",
    "print(\"calibration_config: \", calibration_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ecee868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1627, Accuracy: 9851/10000 (98.51%)\n",
      "\n",
      "Pretrained model Accuracy:  98.51%, Test-time:  1.5880s, Speed-up:  1.07x\n"
     ]
    }
   ],
   "source": [
    "## Performance \n",
    "start = time.time()\n",
    "best_acc = test(model, device)\n",
    "test_time = time.time() - start\n",
    "\n",
    "print(f'Pretrained model Accuracy: {best_acc: .2f}%, Test-time: {test_time: .4f}s, Speed-up: {pre_test_time/test_time: .2f}x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
